{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from netCDF4 import num2date\n",
    "\n",
    "file_path = \"IBTrACS.WP.v04r01.nc\"\n",
    "ds = nc.Dataset(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iso_time_raw = ds.variables['iso_time'][:]  \n",
    "iso_time_str = np.array([''.join(ch.decode('utf-8') for ch in row).strip() if not np.ma.is_masked(row) else '' for row in iso_time_raw.reshape(-1, iso_time_raw.shape[2])])\n",
    "\n",
    "# print(iso_time_str[:2])  \n",
    "\n",
    "iso_time_parsed = pd.to_datetime(iso_time_str, errors='coerce')\n",
    "\n",
    "# print(iso_time_parsed[:2])  \n",
    "\n",
    "gpm_start = pd.Timestamp(\"2000-06-01T00:00:00\")\n",
    "gpm_end = pd.Timestamp(\"2025-02-21T12:30:00\")\n",
    "\n",
    "valid_time_mask = (iso_time_parsed >= gpm_start) & (iso_time_parsed <= gpm_end)\n",
    "\n",
    "sid_raw = ds.variables['sid'][:]  \n",
    "sid_str = [''.join([ch.decode('utf-8') for ch in sid_raw[i, :]]).strip() for i in range(sid_raw.shape[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[cftime.DatetimeGregorian(1884, 6, 24, 16, 0, 0, 27, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 24, 18, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 24, 21, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 25, 0, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 25, 3, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 25, 4, 0, 0, 27, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 25, 6, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 25, 9, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 25, 12, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 25, 15, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 25, 16, 0, 0, 27, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 25, 18, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 25, 21, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 26, 0, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 26, 3, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 26, 4, 0, 0, 27, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 26, 6, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 26, 9, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 26, 12, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 26, 15, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 26, 16, 0, 0, 27, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 26, 18, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 26, 21, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 27, 0, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 27, 3, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 27, 4, 0, 0, 27, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 27, 6, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 27, 9, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 27, 12, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 27, 15, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 27, 16, 0, 0, 27, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 27, 18, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 27, 21, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 28, 0, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 28, 3, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 28, 4, 0, 0, 27, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 28, 6, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 28, 9, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 28, 12, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 28, 15, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 28, 16, 0, 0, 27, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 28, 18, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 28, 21, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 29, 0, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 29, 3, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 29, 4, 0, 0, 27, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 29, 6, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 29, 9, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 29, 12, 0, 0, 40, has_year_zero=False)\n",
      "  cftime.DatetimeGregorian(1884, 6, 29, 15, 0, 0, 40, has_year_zero=False)\n",
      "  -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
      "  -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
      "  -- --]]\n",
      "1884177N17124\n"
     ]
    }
   ],
   "source": [
    "time_var = ds.variables['time'][:]\n",
    "\n",
    "time_units = ds.variables['time'].units\n",
    "time_calendar = ds.variables['time'].calendar if 'calendar' in ds.variables['time'].ncattrs() else 'standard'\n",
    "\n",
    "# to datetime\n",
    "iso_time_dt = num2date(time_var, time_units, calendar=time_calendar)\n",
    "\n",
    "# print(iso_time_dt[0:1])  \n",
    "\n",
    "# GPM range\n",
    "gpm_start = pd.Timestamp(\"2000-06-01T00:00:00\")\n",
    "gpm_end = pd.Timestamp(\"2025-02-21T12:30:00\")\n",
    "\n",
    "valid_time_mask = (iso_time_dt >= gpm_start) & (iso_time_dt <= gpm_end)\n",
    "\n",
    "# strom id\n",
    "sid_raw = ds.variables['sid'][:]  \n",
    "# turn to string\n",
    "sid_str = [''.join([ch.decode('utf-8') for ch in sid_raw[i, :]]).strip() for i in range(sid_raw.shape[0])]\n",
    "# print(sid_str[0])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manully select the variables from usa data （Their fill value is the same -9999.0）\n",
    "# I use the merged latitudes and longitudes, strom speed and direction from IBTrACS to make GPM data align with data from all sources\n",
    "# 'usa_gust'(max speed of gust) and 'usa_eye'(eye diameter) are always NaN, I am not sure if they are important, but finally threw them away\n",
    "variables_to_extract = [\n",
    "    'usa_lat', 'usa_lon', 'storm_dir', 'storm_speed', 'usa_wind', 'usa_pres', 'usa_poci', 'usa_roci', \n",
    "    'usa_rmw','usa_eye'\n",
    "]\n",
    "\n",
    "filtered_data = {'sid': [], 'time': []}\n",
    "\n",
    "for var_name in variables_to_extract:\n",
    "    filtered_data[var_name] = []\n",
    "\n",
    "for storm_idx, sid in enumerate(sid_str):\n",
    "    time_mask = valid_time_mask[storm_idx, :]  \n",
    "\n",
    "# time \n",
    "    valid_times = iso_time_dt[storm_idx, time_mask]\n",
    "    filtered_data['sid'].extend([sid] * len(valid_times))\n",
    "    filtered_data['time'].extend(valid_times)\n",
    "\n",
    "    # add parameters' data      \n",
    "    for var_name in variables_to_extract:\n",
    "        var_data = ds.variables[var_name][:]\n",
    "        fill_value = ds.variables[var_name].getncattr('_FillValue') if '_FillValue' in ds.variables[var_name].ncattrs() else None\n",
    "\n",
    "        filtered_values = var_data[storm_idx, time_mask]\n",
    "\n",
    "        if fill_value is not None:\n",
    "            filtered_values = np.where(filtered_values == fill_value, np.nan, filtered_values)\n",
    "        else:\n",
    "            filtered_values = np.where(filtered_values == -12345.0, np.nan, filtered_values)\n",
    "        filtered_values = np.array(filtered_values, dtype=float)\n",
    "        filtered_values = filtered_values.astype(float)                    \n",
    "        filtered_values = np.round(filtered_values, 6)\n",
    "        filtered_data[var_name].extend(filtered_values)\n",
    "\n",
    "filtered_df = pd.DataFrame(filtered_data)\n",
    "filtered_df.to_csv(\"filtered_storms.csv\", index=False, na_rep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iso_time for filtering, time-costing\n",
    "# manully select the variables from usa data （Their fill value is the same -9999.0）\n",
    "# I use the merged latitudes and longitudes, strom speed and direction from IBTrACS to make GPM data align with data from all sources\n",
    "# 'usa_gust'(max speed of gust) and 'usa_eye'(eye diameter) are always NaN, I am not sure if they are important, but finally threw them away\n",
    "variables_to_extract = [\n",
    "    'usa_lat', 'usa_lon', 'storm_dir', 'storm_speed', 'usa_wind', 'usa_pres', 'usa_poci', 'usa_roci', \n",
    "    'usa_rmw'\n",
    "]\n",
    "filtered_data = {'sid': [], 'time': []}\n",
    "\n",
    "for var_name in variables_to_extract:\n",
    "    filtered_data[var_name] = []\n",
    "for storm_idx, sid in enumerate(sid_str):\n",
    "    time_mask = (iso_time_parsed >= gpm_start) & (iso_time_parsed <= gpm_end)\n",
    "    \n",
    "    # time \n",
    "    valid_times = iso_time_parsed[time_mask]\n",
    "    filtered_data['sid'].extend([sid] * len(valid_times))\n",
    "    filtered_data['time'].extend(valid_times)\n",
    "\n",
    "    # add parameters' data      \n",
    "    for var_name in variables_to_extract:\n",
    "        var_data = ds.variables[var_name][:]\n",
    "        fill_value = ds.variables[var_name].getncattr('_FillValue') if '_FillValue' in ds.variables[var_name].ncattrs() else None\n",
    "\n",
    "        filtered_values = var_data[storm_idx, :][time_mask[:var_data.shape[1]]]\n",
    "\n",
    "        if fill_value is not None:\n",
    "            filtered_values = np.where(filtered_values == fill_value, np.nan, filtered_values)\n",
    "        else:\n",
    "            filtered_values = np.where(filtered_values == -12345.0, np.nan, filtered_values)\n",
    "        filtered_values = np.array(filtered_values, dtype=float)\n",
    "        filtered_values = filtered_values.astype(float)                    \n",
    "        filtered_values = np.round(filtered_values, 6)\n",
    "        filtered_data[var_name].extend(filtered_values)\n",
    "\n",
    "filtered_df = pd.DataFrame(filtered_data)\n",
    "filtered_df.to_csv(\"filtered_storms.csv\", index=False, na_rep=\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                sid                        time  usa_lat     usa_lon  \\\n",
      "2370  2001204N19127  2001-07-24 00:00:00.000040     20.4  118.300003   \n",
      "2371  2001204N19127  2001-07-24 03:00:00.000040     20.5  117.599998   \n",
      "2372  2001204N19127  2001-07-24 06:00:00.000040     20.5  116.900002   \n",
      "2373  2001204N19127  2001-07-24 09:00:00.000040     20.5  116.300003   \n",
      "2374  2001204N19127  2001-07-24 12:00:00.000040     20.6  115.699997   \n",
      "\n",
      "      storm_dir  storm_speed  usa_wind  usa_pres  usa_poci  usa_roci  usa_rmw  \\\n",
      "2370      280.0         14.0      55.0     984.0    1005.0      52.0     50.0   \n",
      "2371      275.0         13.0      63.0     978.0    1005.0      79.0     45.0   \n",
      "2372      275.0         12.0      70.0     972.0    1005.0     105.0     40.0   \n",
      "2373      275.0         11.0      78.0     965.0    1005.0     120.0     38.0   \n",
      "2374      275.0         11.0      85.0     958.0    1005.0     135.0     35.0   \n",
      "\n",
      "      usa_eye  distance_to_shenzhen  \n",
      "2370  -9999.0            499.432200  \n",
      "2371  -9999.0            431.087449  \n",
      "2372  -9999.0            371.521671  \n",
      "2373  -9999.0            324.643423  \n",
      "2374  -9999.0            274.794829  \n"
     ]
    }
   ],
   "source": [
    "# Haversine formula\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = 6371 * c  \n",
    "    return km\n",
    "\n",
    "shenzhen_lat = 22.5431\n",
    "shenzhen_lon = 114.0579\n",
    "\n",
    "filtered_df['distance_to_shenzhen'] = haversine(filtered_df['usa_lon'], filtered_df['usa_lat'], \n",
    "                                                shenzhen_lon, shenzhen_lat)\n",
    "\n",
    "filtered_df_shenzhen = filtered_df[filtered_df['distance_to_shenzhen'] <= 500]\n",
    "\n",
    "\n",
    "filtered_df_shenzhen.to_csv(\"filtered_storms_shenzhen_500km.csv\", index=False)\n",
    "\n",
    "# Delete observations without full USA values\n",
    "cols_to_check = ['usa_wind', 'usa_pres', 'usa_poci', 'usa_roci', 'usa_rmw']\n",
    "\n",
    "filtered_df_deleted = filtered_df_shenzhen[~(filtered_df_shenzhen[cols_to_check] == -9999.0).any(axis=1)]\n",
    "filtered_df_deleted.to_csv(\"filtered_storms_deleted.csv\", index=False)\n",
    "\n",
    "print(filtered_df_deleted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q3/zyn4kqx13z382l5096z1c4fw0000gn/T/ipykernel_10726/2094076902.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df_deleted['time'] = filtered_df_deleted['time'].astype(str)\n",
      "/var/folders/q3/zyn4kqx13z382l5096z1c4fw0000gn/T/ipykernel_10726/2094076902.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df_deleted['time'] = pd.to_datetime(filtered_df_deleted['time'], errors='coerce')\n",
      "/var/folders/q3/zyn4kqx13z382l5096z1c4fw0000gn/T/ipykernel_10726/2094076902.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df_deleted['time'] = filtered_df_deleted['time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n"
     ]
    }
   ],
   "source": [
    "# Convert time to standard format for submitting to GEE\n",
    "filtered_df_deleted['time'] = filtered_df_deleted['time'].astype(str)\n",
    "filtered_df_deleted['time'] = pd.to_datetime(filtered_df_deleted['time'], errors='coerce')\n",
    "# yyyy-MM-dd HH:mm:ss\n",
    "filtered_df_deleted['time'] = filtered_df_deleted['time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "filtered_df_deleted.to_csv(\"filtered_storms_deleted_formatted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JAPAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manully select the variables from usa data （Their fill value is the same -9999.0）\n",
    "# I use the merged latitudes and longitudes, strom speed and direction from IBTrACS to make GPM data align with data from all sources\n",
    "# 'usa_gust'(max speed of gust) and 'usa_eye'(eye diameter) are always NaN, I am not sure if they are important, but finally threw them away\n",
    "variables_to_extract = ['lat','lon',\n",
    "    'tokyo_lat', 'tokyo_lon','storm_dir', 'storm_speed', 'tokyo_grade', 'tokyo_wind', 'tokyo_pres',\n",
    "    'tokyo_r50_dir', 'tokyo_r50_long', 'tokyo_r50_short', 'tokyo_r30_dir', 'tokyo_r30_long', \n",
    "    'tokyo_r30_short', 'tokyo_land'\n",
    "]\n",
    "filtered_data = {'sid': [], 'time': []}\n",
    "\n",
    "for var_name in variables_to_extract:\n",
    "    filtered_data[var_name] = []\n",
    "\n",
    "for storm_idx, sid in enumerate(sid_str):\n",
    "    time_mask = valid_time_mask[storm_idx, :]  \n",
    "\n",
    "# time \n",
    "    valid_times = iso_time_dt[storm_idx, time_mask]\n",
    "    filtered_data['sid'].extend([sid] * len(valid_times))\n",
    "    filtered_data['time'].extend(valid_times)\n",
    "\n",
    "    # add parameters' data      \n",
    "    for var_name in variables_to_extract:\n",
    "        var_data = ds.variables[var_name][:]\n",
    "        fill_value = ds.variables[var_name].getncattr('_FillValue') if '_FillValue' in ds.variables[var_name].ncattrs() else None\n",
    "\n",
    "        filtered_values = var_data[storm_idx, time_mask]\n",
    "\n",
    "        if fill_value is not None:\n",
    "            filtered_values = np.where(filtered_values == fill_value, np.nan, filtered_values)\n",
    "        else:\n",
    "            filtered_values = np.where(filtered_values == -12345.0, np.nan, filtered_values)\n",
    "        filtered_values = np.array(filtered_values, dtype=float)\n",
    "        filtered_values = filtered_values.astype(float)                    \n",
    "        filtered_values = np.round(filtered_values, 6)\n",
    "        filtered_data[var_name].extend(filtered_values)\n",
    "\n",
    "filtered_df = pd.DataFrame(filtered_data)\n",
    "\n",
    "filtered_df.to_csv(\"storms_JPN.csv\", index=False, na_rep=\"\")\n",
    "\n",
    "filtered_df['distance_to_shenzhen'] = haversine(filtered_df['lon'], filtered_df['lat'], \n",
    "                                                shenzhen_lon, shenzhen_lat)\n",
    "\n",
    "\n",
    "filtered_df_shenzhen = filtered_df[filtered_df['distance_to_shenzhen'] <= 500]\n",
    "\n",
    "# Delete observations without full USA values\n",
    "cols_to_check = ['tokyo_wind', 'tokyo_pres']\n",
    "\n",
    "# This standard: those with all JPN values missing are deleted, then number of observations: 2403\n",
    "# filtered_df_deleted = filtered_df_gd[~(filtered_df_gd[cols_to_check] == -9999.0).all(axis=1)]\n",
    "\n",
    "# Then new standard: those with at least one JPN values missing are deleted\n",
    "# number of observations :503\n",
    "\n",
    "filtered_df_deleted = filtered_df_shenzhen[~(filtered_df_shenzhen[cols_to_check] == -9999.0).any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q3/zyn4kqx13z382l5096z1c4fw0000gn/T/ipykernel_10726/2864077730.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df_deleted['time'] = filtered_df_deleted['time'].astype(str)\n",
      "/var/folders/q3/zyn4kqx13z382l5096z1c4fw0000gn/T/ipykernel_10726/2864077730.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df_deleted['time'] = pd.to_datetime(filtered_df_deleted['time'], errors='coerce')\n",
      "/var/folders/q3/zyn4kqx13z382l5096z1c4fw0000gn/T/ipykernel_10726/2864077730.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df_deleted['time'] = filtered_df_deleted['time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n"
     ]
    }
   ],
   "source": [
    "# Convert time to standard format for submitting to GEE\n",
    "filtered_df_deleted['time'] = filtered_df_deleted['time'].astype(str)\n",
    "filtered_df_deleted['time'] = pd.to_datetime(filtered_df_deleted['time'], errors='coerce')\n",
    "# yyyy-MM-dd HH:mm:ss\n",
    "filtered_df_deleted['time'] = filtered_df_deleted['time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "filtered_df_deleted.to_csv(\"storms_JPN_formatted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CMA(CN) has only lat, lon, wind and pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q3/zyn4kqx13z382l5096z1c4fw0000gn/T/ipykernel_10726/637922043.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df_deleted['time'] = filtered_df_deleted['time'].astype(str)\n",
      "/var/folders/q3/zyn4kqx13z382l5096z1c4fw0000gn/T/ipykernel_10726/637922043.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df_deleted['time'] = pd.to_datetime(filtered_df_deleted['time'], errors='coerce')\n",
      "/var/folders/q3/zyn4kqx13z382l5096z1c4fw0000gn/T/ipykernel_10726/637922043.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df_deleted['time'] = filtered_df_deleted['time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "variables_to_extract = ['lat','lon',\n",
    "'cma_lat', 'cma_lon','storm_dir', 'storm_speed', 'cma_wind', 'cma_pres']\n",
    "filtered_data = {'sid': [], 'time': []}\n",
    "\n",
    "for var_name in variables_to_extract:\n",
    "    filtered_data[var_name] = []\n",
    "\n",
    "for storm_idx, sid in enumerate(sid_str):\n",
    "    time_mask = valid_time_mask[storm_idx, :]  \n",
    "\n",
    "# time \n",
    "    valid_times = iso_time_dt[storm_idx, time_mask]\n",
    "    filtered_data['sid'].extend([sid] * len(valid_times))\n",
    "    filtered_data['time'].extend(valid_times)\n",
    "\n",
    "    # add parameters' data      \n",
    "    for var_name in variables_to_extract:\n",
    "        var_data = ds.variables[var_name][:]\n",
    "        fill_value = ds.variables[var_name].getncattr('_FillValue') if '_FillValue' in ds.variables[var_name].ncattrs() else None\n",
    "\n",
    "        filtered_values = var_data[storm_idx, time_mask]\n",
    "\n",
    "        \n",
    "        filtered_values = np.where(filtered_values == fill_value, np.nan, filtered_values)\n",
    "        filtered_values = np.array(filtered_values, dtype=float)\n",
    "        filtered_values = filtered_values.astype(float)                    \n",
    "        filtered_values = np.round(filtered_values, 6)\n",
    "        filtered_data[var_name].extend(filtered_values)\n",
    "\n",
    "filtered_df = pd.DataFrame(filtered_data)\n",
    "\n",
    "filtered_df.to_csv(\"storms_cma.csv\", index=False, na_rep=\"\")\n",
    "\n",
    "filtered_df['distance_to_shenzhen'] = haversine(filtered_df['lon'], filtered_df['lat'], \n",
    "                                                shenzhen_lon, shenzhen_lat)\n",
    "\n",
    "\n",
    "filtered_df_shenzhen = filtered_df[filtered_df['distance_to_shenzhen'] <= 500]\n",
    "\n",
    "cols_to_check = ['cma_wind', 'cma_pres']\n",
    "\n",
    "\n",
    "filtered_df_deleted = filtered_df_shenzhen[~(filtered_df_shenzhen[cols_to_check] == -9999.0).any(axis=1)]\n",
    "filtered_df_deleted.to_csv('storms_cma_sz.csv', index=False)\n",
    "\n",
    "filtered_df_deleted['time'] = filtered_df_deleted['time'].astype(str)\n",
    "filtered_df_deleted['time'] = pd.to_datetime(filtered_df_deleted['time'], errors='coerce')\n",
    "# yyyy-MM-dd HH:mm:ss\n",
    "filtered_df_deleted['time'] = filtered_df_deleted['time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "filtered_df_deleted.to_csv(\"storms_cma_formatted.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_extract = ['lat','lon',\n",
    "    'kma_lat', 'kma_lon','storm_dir', 'storm_speed', 'kma_wind', 'kma_pres', 'kma_r50_dir', 'kma_r50_long', 'kma_r50_short', 'kma_r30_dir', 'kma_r30_long', 'kma_r30_short'\n",
    "]\n",
    "filtered_data = {'sid': [], 'time': []}\n",
    "\n",
    "for var_name in variables_to_extract:\n",
    "    filtered_data[var_name] = []\n",
    "\n",
    "for storm_idx, sid in enumerate(sid_str):\n",
    "    time_mask = valid_time_mask[storm_idx, :]  \n",
    "\n",
    "# time \n",
    "    valid_times = iso_time_dt[storm_idx, time_mask]\n",
    "    filtered_data['sid'].extend([sid] * len(valid_times))\n",
    "    filtered_data['time'].extend(valid_times)\n",
    "\n",
    "    # add parameters' data      \n",
    "    for var_name in variables_to_extract:\n",
    "        var_data = ds.variables[var_name][:]\n",
    "        fill_value = ds.variables[var_name].getncattr('_FillValue') if '_FillValue' in ds.variables[var_name].ncattrs() else None\n",
    "\n",
    "        filtered_values = var_data[storm_idx, time_mask]\n",
    "\n",
    "        \n",
    "        filtered_values = np.where(filtered_values == fill_value, np.nan, filtered_values)\n",
    "        filtered_values = np.array(filtered_values, dtype=float)\n",
    "        filtered_values = filtered_values.astype(float)                    \n",
    "        filtered_values = np.round(filtered_values, 6)\n",
    "        filtered_data[var_name].extend(filtered_values)\n",
    "\n",
    "filtered_df = pd.DataFrame(filtered_data)\n",
    "\n",
    "filtered_df.to_csv(\"storms_KMA.csv\", index=False, na_rep=\"\")\n",
    "\n",
    "filtered_df['distance_to_shenzhen'] = haversine(filtered_df['lon'], filtered_df['lat'], \n",
    "                                                shenzhen_lon, shenzhen_lat)\n",
    "\n",
    "\n",
    "filtered_df_shenzhen = filtered_df[filtered_df['distance_to_shenzhen'] <= 500]\n",
    "\n",
    "cols_to_check = ['kma_wind', 'kma_pres']\n",
    "\n",
    "\n",
    "filtered_df_deleted = filtered_df_shenzhen[~(filtered_df_shenzhen[cols_to_check] == -9999.0).any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q3/zyn4kqx13z382l5096z1c4fw0000gn/T/ipykernel_10726/140406962.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df_deleted['time'] = filtered_df_deleted['time'].astype(str)\n",
      "/var/folders/q3/zyn4kqx13z382l5096z1c4fw0000gn/T/ipykernel_10726/140406962.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df_deleted['time'] = pd.to_datetime(filtered_df_deleted['time'], errors='coerce')\n",
      "/var/folders/q3/zyn4kqx13z382l5096z1c4fw0000gn/T/ipykernel_10726/140406962.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df_deleted['time'] = filtered_df_deleted['time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n"
     ]
    }
   ],
   "source": [
    "filtered_df_deleted['time'] = filtered_df_deleted['time'].astype(str)\n",
    "filtered_df_deleted['time'] = pd.to_datetime(filtered_df_deleted['time'], errors='coerce')\n",
    "# yyyy-MM-dd HH:mm:ss\n",
    "filtered_df_deleted['time'] = filtered_df_deleted['time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "filtered_df_deleted.to_csv(\"storms_KMA_formatted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Delhi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "variables_to_extract = ['lat','lon',\n",
    "    'newdelhi_lat', 'newdelhi_lon','storm_dir', 'storm_speed', \n",
    "    'newdelhi_wind', 'newdelhi_pres', 'newdelhi_dp', 'newdelhi_poci']\n",
    "filtered_data = {'sid': [], 'time': []}\n",
    "\n",
    "for var_name in variables_to_extract:\n",
    "    filtered_data[var_name] = []\n",
    "\n",
    "for storm_idx, sid in enumerate(sid_str):\n",
    "    time_mask = valid_time_mask[storm_idx, :]  \n",
    "\n",
    "# time \n",
    "    valid_times = iso_time_dt[storm_idx, time_mask]\n",
    "    filtered_data['sid'].extend([sid] * len(valid_times))\n",
    "    filtered_data['time'].extend(valid_times)\n",
    "\n",
    "    # add parameters' data      \n",
    "    for var_name in variables_to_extract:\n",
    "        var_data = ds.variables[var_name][:]\n",
    "        fill_value = ds.variables[var_name].getncattr('_FillValue') if '_FillValue' in ds.variables[var_name].ncattrs() else None\n",
    "\n",
    "        filtered_values = var_data[storm_idx, time_mask]\n",
    "\n",
    "        \n",
    "        filtered_values = np.where(filtered_values == fill_value, np.nan, filtered_values)\n",
    "        filtered_values = np.array(filtered_values, dtype=float)\n",
    "        filtered_values = filtered_values.astype(float)                    \n",
    "        filtered_values = np.round(filtered_values, 6)\n",
    "        filtered_data[var_name].extend(filtered_values)\n",
    "\n",
    "filtered_df = pd.DataFrame(filtered_data)\n",
    "\n",
    "filtered_df.to_csv(\"storms_Ndl.csv\", index=False, na_rep=\"\")\n",
    "\n",
    "filtered_df['distance_to_shenzhen'] = haversine(filtered_df['lon'], filtered_df['lat'], \n",
    "                                                shenzhen_lon, shenzhen_lat)\n",
    "\n",
    "\n",
    "filtered_df_shenzhen = filtered_df[filtered_df['distance_to_shenzhen'] <= 500]\n",
    "\n",
    "cols_to_check = [ \n",
    "    'newdelhi_wind', 'newdelhi_pres', 'newdelhi_dp', 'newdelhi_poci']\n",
    "\n",
    "\n",
    "filtered_df_deleted = filtered_df_shenzhen[~(filtered_df_shenzhen[cols_to_check] == -9999.0).any(axis=1)]\n",
    "\n",
    "filtered_df_deleted['time'] = filtered_df_deleted['time'].astype(str)\n",
    "filtered_df_deleted['time'] = pd.to_datetime(filtered_df_deleted['time'], errors='coerce')\n",
    "# yyyy-MM-dd HH:mm:ss\n",
    "filtered_df_deleted['time'] = filtered_df_deleted['time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "filtered_df_deleted.to_csv(\"storms_Ndl_formatted.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LA Reunion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "variables_to_extract = ['lat','lon',\n",
    "'reunion_lat', 'reunion_lon','storm_dir', 'storm_speed', \n",
    "'reunion_pres', 'reunion_wind','reunion_rmw','reunion_gust']\n",
    "filtered_data = {'sid': [], 'time': []}\n",
    "\n",
    "for var_name in variables_to_extract:\n",
    "    filtered_data[var_name] = []\n",
    "\n",
    "for storm_idx, sid in enumerate(sid_str):\n",
    "    time_mask = valid_time_mask[storm_idx, :]  \n",
    "\n",
    "# time \n",
    "    valid_times = iso_time_dt[storm_idx, time_mask]\n",
    "    filtered_data['sid'].extend([sid] * len(valid_times))\n",
    "    filtered_data['time'].extend(valid_times)\n",
    "\n",
    "    # add parameters' data      \n",
    "    for var_name in variables_to_extract:\n",
    "        var_data = ds.variables[var_name][:]\n",
    "        fill_value = ds.variables[var_name].getncattr('_FillValue') if '_FillValue' in ds.variables[var_name].ncattrs() else None\n",
    "\n",
    "        filtered_values = var_data[storm_idx, time_mask]\n",
    "\n",
    "        \n",
    "        filtered_values = np.where(filtered_values == fill_value, np.nan, filtered_values)\n",
    "        filtered_values = np.array(filtered_values, dtype=float)\n",
    "        filtered_values = filtered_values.astype(float)                    \n",
    "        filtered_values = np.round(filtered_values, 6)\n",
    "        filtered_data[var_name].extend(filtered_values)\n",
    "\n",
    "filtered_df = pd.DataFrame(filtered_data)\n",
    "\n",
    "filtered_df.to_csv(\"storms_reunion.csv\", index=False, na_rep=\"\")\n",
    "\n",
    "filtered_df['distance_to_shenzhen'] = haversine(filtered_df['lon'], filtered_df['lat'], \n",
    "                                                shenzhen_lon, shenzhen_lat)\n",
    "\n",
    "\n",
    "filtered_df_shenzhen = filtered_df[filtered_df['distance_to_shenzhen'] <= 500]\n",
    "\n",
    "cols_to_check = [ 'reunion_wind'\n",
    "'reunion_pres', 'reunion_rmw','reunion_gust']\n",
    "\n",
    "\n",
    "filtered_df_deleted = filtered_df_shenzhen[~(filtered_df_shenzhen[cols_to_check] == -9999.0).any(axis=1)]\n",
    "filtered_df_deleted.to_csv('storms_reunion_sz.csv', index=False)\n",
    "\n",
    "filtered_df_deleted['time'] = filtered_df_deleted['time'].astype(str)\n",
    "filtered_df_deleted['time'] = pd.to_datetime(filtered_df_deleted['time'], errors='coerce')\n",
    "# yyyy-MM-dd HH:mm:ss\n",
    "filtered_df_deleted['time'] = filtered_df_deleted['time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "filtered_df_deleted.to_csv(\"storms_reunion_formatted.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_extract = ['lat','lon',\n",
    "'bom_lat', 'bom_lon','storm_dir', 'storm_speed', \n",
    "'bom_wind', 'bom_pres','bom_rmw', 'bom_roci', 'bom_poci', 'bom_eye', 'bom_gust']\n",
    "filtered_data = {'sid': [], 'time': []}\n",
    "\n",
    "for var_name in variables_to_extract:\n",
    "    filtered_data[var_name] = []\n",
    "\n",
    "for storm_idx, sid in enumerate(sid_str):\n",
    "    time_mask = valid_time_mask[storm_idx, :]  \n",
    "\n",
    "# time \n",
    "    valid_times = iso_time_dt[storm_idx, time_mask]\n",
    "    filtered_data['sid'].extend([sid] * len(valid_times))\n",
    "    filtered_data['time'].extend(valid_times)\n",
    "\n",
    "    # add parameters' data      \n",
    "    for var_name in variables_to_extract:\n",
    "        var_data = ds.variables[var_name][:]\n",
    "        fill_value = ds.variables[var_name].getncattr('_FillValue') if '_FillValue' in ds.variables[var_name].ncattrs() else None\n",
    "\n",
    "        filtered_values = var_data[storm_idx, time_mask]\n",
    "\n",
    "        \n",
    "        filtered_values = np.where(filtered_values == fill_value, np.nan, filtered_values)\n",
    "        filtered_values = np.array(filtered_values, dtype=float)\n",
    "        filtered_values = filtered_values.astype(float)                    \n",
    "        filtered_values = np.round(filtered_values, 6)\n",
    "        filtered_data[var_name].extend(filtered_values)\n",
    "\n",
    "filtered_df = pd.DataFrame(filtered_data)\n",
    "\n",
    "filtered_df.to_csv(\"storms_bom.csv\", index=False, na_rep=\"\")\n",
    "\n",
    "filtered_df['distance_to_shenzhen'] = haversine(filtered_df['lon'], filtered_df['lat'], \n",
    "                                                shenzhen_lon, shenzhen_lat)\n",
    "\n",
    "\n",
    "filtered_df_shenzhen = filtered_df[filtered_df['distance_to_shenzhen'] <= 500]\n",
    "\n",
    "cols_to_check = [ \n",
    "'bom_wind', 'bom_pres','bom_rmw', 'bom_roci', 'bom_poci', 'bom_eye', 'bom_gust']\n",
    "\n",
    "\n",
    "filtered_df_deleted = filtered_df_shenzhen[~(filtered_df_shenzhen[cols_to_check] == -9999.0).any(axis=1)]\n",
    "filtered_df_deleted.to_csv('storms_bom_sz.csv', index=False)\n",
    "\n",
    "filtered_df_deleted['time'] = filtered_df_deleted['time'].astype(str)\n",
    "filtered_df_deleted['time'] = pd.to_datetime(filtered_df_deleted['time'], errors='coerce')\n",
    "# yyyy-MM-dd HH:mm:ss\n",
    "filtered_df_deleted['time'] = filtered_df_deleted['time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "filtered_df_deleted.to_csv(\"storms_bom_formatted.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nadi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "variables_to_extract = ['lat','lon',\n",
    "'nadi_lat', 'nadi_lon','storm_dir', 'storm_speed', 'nadi_wind', 'nadi_pres']\n",
    "filtered_data = {'sid': [], 'time': []}\n",
    "\n",
    "for var_name in variables_to_extract:\n",
    "    filtered_data[var_name] = []\n",
    "\n",
    "for storm_idx, sid in enumerate(sid_str):\n",
    "    time_mask = valid_time_mask[storm_idx, :]  \n",
    "\n",
    "# time \n",
    "    valid_times = iso_time_dt[storm_idx, time_mask]\n",
    "    filtered_data['sid'].extend([sid] * len(valid_times))\n",
    "    filtered_data['time'].extend(valid_times)\n",
    "\n",
    "    # add parameters' data      \n",
    "    for var_name in variables_to_extract:\n",
    "        var_data = ds.variables[var_name][:]\n",
    "        fill_value = ds.variables[var_name].getncattr('_FillValue') if '_FillValue' in ds.variables[var_name].ncattrs() else None\n",
    "\n",
    "        filtered_values = var_data[storm_idx, time_mask]\n",
    "\n",
    "        \n",
    "        filtered_values = np.where(filtered_values == fill_value, np.nan, filtered_values)\n",
    "        filtered_values = np.array(filtered_values, dtype=float)\n",
    "        filtered_values = filtered_values.astype(float)                    \n",
    "        filtered_values = np.round(filtered_values, 6)\n",
    "        filtered_data[var_name].extend(filtered_values)\n",
    "\n",
    "filtered_df = pd.DataFrame(filtered_data)\n",
    "\n",
    "filtered_df.to_csv(\"storms_nadi.csv\", index=False, na_rep=\"\")\n",
    "\n",
    "filtered_df['distance_to_shenzhen'] = haversine(filtered_df['lon'], filtered_df['lat'], \n",
    "                                                shenzhen_lon, shenzhen_lat)\n",
    "\n",
    "\n",
    "filtered_df_shenzhen = filtered_df[filtered_df['distance_to_shenzhen'] <= 500]\n",
    "\n",
    "cols_to_check = ['nadi_wind', 'nadi_pres']\n",
    "\n",
    "\n",
    "filtered_df_deleted = filtered_df_shenzhen[~(filtered_df_shenzhen[cols_to_check] == -9999.0).any(axis=1)]\n",
    "filtered_df_deleted.to_csv('storms_nadi_sz.csv', index=False)\n",
    "\n",
    "filtered_df_deleted['time'] = filtered_df_deleted['time'].astype(str)\n",
    "filtered_df_deleted['time'] = pd.to_datetime(filtered_df_deleted['time'], errors='coerce')\n",
    "# yyyy-MM-dd HH:mm:ss\n",
    "filtered_df_deleted['time'] = filtered_df_deleted['time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "filtered_df_deleted.to_csv(\"storms_nadi_formatted.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newzealand  Wellington"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "variables_to_extract = ['lat','lon',\n",
    "'wellington_lat', 'wellington_lon', 'storm_dir', 'storm_speed', 'wellington_wind', 'wellington_pres']\n",
    "filtered_data = {'sid': [], 'time': []}\n",
    "\n",
    "for var_name in variables_to_extract:\n",
    "    filtered_data[var_name] = []\n",
    "\n",
    "for storm_idx, sid in enumerate(sid_str):\n",
    "    time_mask = valid_time_mask[storm_idx, :]  \n",
    "\n",
    "# time \n",
    "    valid_times = iso_time_dt[storm_idx, time_mask]\n",
    "    filtered_data['sid'].extend([sid] * len(valid_times))\n",
    "    filtered_data['time'].extend(valid_times)\n",
    "\n",
    "    # add parameters' data      \n",
    "    for var_name in variables_to_extract:\n",
    "        var_data = ds.variables[var_name][:]\n",
    "        fill_value = ds.variables[var_name].getncattr('_FillValue') if '_FillValue' in ds.variables[var_name].ncattrs() else None\n",
    "\n",
    "        filtered_values = var_data[storm_idx, time_mask]\n",
    "\n",
    "        \n",
    "        filtered_values = np.where(filtered_values == fill_value, np.nan, filtered_values)\n",
    "        filtered_values = np.array(filtered_values, dtype=float)\n",
    "        filtered_values = filtered_values.astype(float)                    \n",
    "        filtered_values = np.round(filtered_values, 6)\n",
    "        filtered_data[var_name].extend(filtered_values)\n",
    "\n",
    "filtered_df = pd.DataFrame(filtered_data)\n",
    "\n",
    "filtered_df.to_csv(\"storms_nz.csv\", index=False, na_rep=\"\")\n",
    "\n",
    "filtered_df['distance_to_shenzhen'] = haversine(filtered_df['lon'], filtered_df['lat'], \n",
    "                                                shenzhen_lon, shenzhen_lat)\n",
    "\n",
    "\n",
    "filtered_df_shenzhen = filtered_df[filtered_df['distance_to_shenzhen'] <= 500]\n",
    "\n",
    "cols_to_check = ['wellington_wind', 'wellington_pres']\n",
    "\n",
    "\n",
    "filtered_df_deleted = filtered_df_shenzhen[~(filtered_df_shenzhen[cols_to_check] == -9999.0).any(axis=1)]\n",
    "filtered_df_deleted.to_csv('storms_nz_sz.csv', index=False)\n",
    "\n",
    "filtered_df_deleted['time'] = filtered_df_deleted['time'].astype(str)\n",
    "filtered_df_deleted['time'] = pd.to_datetime(filtered_df_deleted['time'], errors='coerce')\n",
    "# yyyy-MM-dd HH:mm:ss\n",
    "filtered_df_deleted['time'] = filtered_df_deleted['time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "filtered_df_deleted.to_csv(\"storms_nz_formatted.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "variables_to_extract = ['lat','lon',\n",
    "'ds824_lat', 'ds824_lon','storm_dir', 'storm_speed', 'ds824_wind', 'ds824_pres']\n",
    "filtered_data = {'sid': [], 'time': []}\n",
    "\n",
    "for var_name in variables_to_extract:\n",
    "    filtered_data[var_name] = []\n",
    "\n",
    "for storm_idx, sid in enumerate(sid_str):\n",
    "    time_mask = valid_time_mask[storm_idx, :]  \n",
    "\n",
    "# time \n",
    "    valid_times = iso_time_dt[storm_idx, time_mask]\n",
    "    filtered_data['sid'].extend([sid] * len(valid_times))\n",
    "    filtered_data['time'].extend(valid_times)\n",
    "\n",
    "    # add parameters' data      \n",
    "    for var_name in variables_to_extract:\n",
    "        var_data = ds.variables[var_name][:]\n",
    "        fill_value = ds.variables[var_name].getncattr('_FillValue') if '_FillValue' in ds.variables[var_name].ncattrs() else None\n",
    "\n",
    "        filtered_values = var_data[storm_idx, time_mask]\n",
    "\n",
    "        \n",
    "        filtered_values = np.where(filtered_values == fill_value, np.nan, filtered_values)\n",
    "        filtered_values = np.array(filtered_values, dtype=float)\n",
    "        filtered_values = filtered_values.astype(float)                    \n",
    "        filtered_values = np.round(filtered_values, 6)\n",
    "        filtered_data[var_name].extend(filtered_values)\n",
    "\n",
    "filtered_df = pd.DataFrame(filtered_data)\n",
    "\n",
    "filtered_df.to_csv(\"storms_ds824.csv\", index=False, na_rep=\"\")\n",
    "\n",
    "filtered_df['distance_to_shenzhen'] = haversine(filtered_df['lon'], filtered_df['lat'], \n",
    "                                                shenzhen_lon, shenzhen_lat)\n",
    "\n",
    "\n",
    "filtered_df_shenzhen = filtered_df[filtered_df['distance_to_shenzhen'] <= 500]\n",
    "\n",
    "cols_to_check = ['ds824_wind', 'ds824_pres']\n",
    "\n",
    "\n",
    "filtered_df_deleted = filtered_df_shenzhen[~(filtered_df_shenzhen[cols_to_check] == -9999.0).any(axis=1)]\n",
    "filtered_df_deleted.to_csv('storms_ds824_sz.csv', index=False)\n",
    "\n",
    "filtered_df_deleted['time'] = filtered_df_deleted['time'].astype(str)\n",
    "filtered_df_deleted['time'] = pd.to_datetime(filtered_df_deleted['time'], errors='coerce')\n",
    "# yyyy-MM-dd HH:mm:ss\n",
    "filtered_df_deleted['time'] = filtered_df_deleted['time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "filtered_df_deleted.to_csv(\"storms_ds824_formatted.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['td9636_lat', 'td9636_lon', 'td9636_stage', 'td9636_wind', 'td9636_pres']\n",
    "\n",
    "variables_to_extract = ['lat','lon',\n",
    "'td9636_lat', 'td9636_lon','storm_dir', 'storm_speed', 'td9636_wind', 'td9636_pres']\n",
    "filtered_data = {'sid': [], 'time': []}\n",
    "\n",
    "for var_name in variables_to_extract:\n",
    "    filtered_data[var_name] = []\n",
    "\n",
    "for storm_idx, sid in enumerate(sid_str):\n",
    "    time_mask = valid_time_mask[storm_idx, :]  \n",
    "\n",
    "# time \n",
    "    valid_times = iso_time_dt[storm_idx, time_mask]\n",
    "    filtered_data['sid'].extend([sid] * len(valid_times))\n",
    "    filtered_data['time'].extend(valid_times)\n",
    "\n",
    "    # add parameters' data      \n",
    "    for var_name in variables_to_extract:\n",
    "        var_data = ds.variables[var_name][:]\n",
    "        fill_value = ds.variables[var_name].getncattr('_FillValue') if '_FillValue' in ds.variables[var_name].ncattrs() else None\n",
    "\n",
    "        filtered_values = var_data[storm_idx, time_mask]\n",
    "\n",
    "        \n",
    "        filtered_values = np.where(filtered_values == fill_value, np.nan, filtered_values)\n",
    "        filtered_values = np.array(filtered_values, dtype=float)\n",
    "        filtered_values = filtered_values.astype(float)                    \n",
    "        filtered_values = np.round(filtered_values, 6)\n",
    "        filtered_data[var_name].extend(filtered_values)\n",
    "\n",
    "filtered_df = pd.DataFrame(filtered_data)\n",
    "\n",
    "filtered_df.to_csv(\"storms_td9636.csv\", index=False, na_rep=\"\")\n",
    "\n",
    "filtered_df['distance_to_shenzhen'] = haversine(filtered_df['lon'], filtered_df['lat'], \n",
    "                                                shenzhen_lon, shenzhen_lat)\n",
    "\n",
    "\n",
    "filtered_df_shenzhen = filtered_df[filtered_df['distance_to_shenzhen'] <= 500]\n",
    "\n",
    "cols_to_check = ['td9636_wind', 'td9636_pres']\n",
    "\n",
    "\n",
    "filtered_df_deleted = filtered_df_shenzhen[~(filtered_df_shenzhen[cols_to_check] == -9999.0).any(axis=1)]\n",
    "filtered_df_deleted.to_csv('storms_td9636_sz.csv', index=False)\n",
    "\n",
    "filtered_df_deleted['time'] = filtered_df_deleted['time'].astype(str)\n",
    "filtered_df_deleted['time'] = pd.to_datetime(filtered_df_deleted['time'], errors='coerce')\n",
    "# yyyy-MM-dd HH:mm:ss\n",
    "filtered_df_deleted['time'] = filtered_df_deleted['time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "filtered_df_deleted.to_csv(\"storms_td9636_formatted.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['td9635_lat', 'td9635_lon', 'td9635_stage', 'td9635_wind', 'td9635_pres']\n",
    "\n",
    "variables_to_extract = ['lat','lon',\n",
    "'td9635_lat', 'td9635_lon','storm_dir', 'storm_speed', 'td9635_wind', 'td9635_pres']\n",
    "filtered_data = {'sid': [], 'time': []}\n",
    "\n",
    "for var_name in variables_to_extract:\n",
    "    filtered_data[var_name] = []\n",
    "\n",
    "for storm_idx, sid in enumerate(sid_str):\n",
    "    time_mask = valid_time_mask[storm_idx, :]  \n",
    "\n",
    "# time \n",
    "    valid_times = iso_time_dt[storm_idx, time_mask]\n",
    "    filtered_data['sid'].extend([sid] * len(valid_times))\n",
    "    filtered_data['time'].extend(valid_times)\n",
    "\n",
    "    # add parameters' data      \n",
    "    for var_name in variables_to_extract:\n",
    "        var_data = ds.variables[var_name][:]\n",
    "        fill_value = ds.variables[var_name].getncattr('_FillValue') if '_FillValue' in ds.variables[var_name].ncattrs() else None\n",
    "\n",
    "        filtered_values = var_data[storm_idx, time_mask]\n",
    "\n",
    "        \n",
    "        filtered_values = np.where(filtered_values == fill_value, np.nan, filtered_values)\n",
    "        filtered_values = np.array(filtered_values, dtype=float)\n",
    "        filtered_values = filtered_values.astype(float)                    \n",
    "        filtered_values = np.round(filtered_values, 6)\n",
    "        filtered_data[var_name].extend(filtered_values)\n",
    "\n",
    "filtered_df = pd.DataFrame(filtered_data)\n",
    "\n",
    "filtered_df.to_csv(\"storms_td9635.csv\", index=False, na_rep=\"\")\n",
    "\n",
    "filtered_df['distance_to_shenzhen'] = haversine(filtered_df['lon'], filtered_df['lat'], \n",
    "                                                shenzhen_lon, shenzhen_lat)\n",
    "\n",
    "\n",
    "filtered_df_shenzhen = filtered_df[filtered_df['distance_to_shenzhen'] <= 500]\n",
    "\n",
    "cols_to_check = ['td9635_wind', 'td9635_pres']\n",
    "\n",
    "\n",
    "filtered_df_deleted = filtered_df_shenzhen[~(filtered_df_shenzhen[cols_to_check] == -9999.0).any(axis=1)]\n",
    "filtered_df_deleted.to_csv('storms_td9635_sz.csv', index=False)\n",
    "\n",
    "filtered_df_deleted['time'] = filtered_df_deleted['time'].astype(str)\n",
    "filtered_df_deleted['time'] = pd.to_datetime(filtered_df_deleted['time'], errors='coerce')\n",
    "# yyyy-MM-dd HH:mm:ss\n",
    "filtered_df_deleted['time'] = filtered_df_deleted['time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "filtered_df_deleted.to_csv(\"storms_td9635_formatted.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['neumann_lat', 'neumann_lon', 'neumann_class', 'neumann_wind', 'neumann_pres']\n",
    "\n",
    "variables_to_extract = ['lat','lon',\n",
    "'neumann_lat', 'neumann_lon','storm_dir', 'storm_speed', 'neumann_wind', 'neumann_pres']\n",
    "filtered_data = {'sid': [], 'time': []}\n",
    "\n",
    "for var_name in variables_to_extract:\n",
    "    filtered_data[var_name] = []\n",
    "\n",
    "for storm_idx, sid in enumerate(sid_str):\n",
    "    time_mask = valid_time_mask[storm_idx, :]  \n",
    "\n",
    "# time \n",
    "    valid_times = iso_time_dt[storm_idx, time_mask]\n",
    "    filtered_data['sid'].extend([sid] * len(valid_times))\n",
    "    filtered_data['time'].extend(valid_times)\n",
    "\n",
    "    # add parameters' data      \n",
    "    for var_name in variables_to_extract:\n",
    "        var_data = ds.variables[var_name][:]\n",
    "        fill_value = ds.variables[var_name].getncattr('_FillValue') if '_FillValue' in ds.variables[var_name].ncattrs() else None\n",
    "\n",
    "        filtered_values = var_data[storm_idx, time_mask]\n",
    "\n",
    "        \n",
    "        filtered_values = np.where(filtered_values == fill_value, np.nan, filtered_values)\n",
    "        filtered_values = np.array(filtered_values, dtype=float)\n",
    "        filtered_values = filtered_values.astype(float)                    \n",
    "        filtered_values = np.round(filtered_values, 6)\n",
    "        filtered_data[var_name].extend(filtered_values)\n",
    "\n",
    "filtered_df = pd.DataFrame(filtered_data)\n",
    "\n",
    "filtered_df.to_csv(\"storms_neumann.csv\", index=False, na_rep=\"\")\n",
    "\n",
    "filtered_df['distance_to_shenzhen'] = haversine(filtered_df['lon'], filtered_df['lat'], \n",
    "                                                shenzhen_lon, shenzhen_lat)\n",
    "\n",
    "\n",
    "filtered_df_shenzhen = filtered_df[filtered_df['distance_to_shenzhen'] <= 500]\n",
    "\n",
    "cols_to_check = ['neumann_wind', 'neumann_pres']\n",
    "\n",
    "\n",
    "filtered_df_deleted = filtered_df_shenzhen[~(filtered_df_shenzhen[cols_to_check] == -9999.0).any(axis=1)]\n",
    "filtered_df_deleted.to_csv('storms_neumann_sz.csv', index=False)\n",
    "\n",
    "filtered_df_deleted['time'] = filtered_df_deleted['time'].astype(str)\n",
    "filtered_df_deleted['time'] = pd.to_datetime(filtered_df_deleted['time'], errors='coerce')\n",
    "# yyyy-MM-dd HH:mm:ss\n",
    "filtered_df_deleted['time'] = filtered_df_deleted['time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "filtered_df_deleted.to_csv(\"storms_neumann_formatted.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'source_mlc', 'mlc_lat', 'mlc_lon', 'mlc_class', 'mlc_wind', 'mlc_pres'\n",
    "\n",
    "\n",
    "variables_to_extract = ['lat','lon',\n",
    "'mlc_lat', 'mlc_lon', 'storm_dir', 'storm_speed', 'mlc_wind', 'mlc_pres']\n",
    "filtered_data = {'sid': [], 'time': []}\n",
    "\n",
    "for var_name in variables_to_extract:\n",
    "    filtered_data[var_name] = []\n",
    "\n",
    "for storm_idx, sid in enumerate(sid_str):\n",
    "    time_mask = valid_time_mask[storm_idx, :]  \n",
    "\n",
    "# time \n",
    "    valid_times = iso_time_dt[storm_idx, time_mask]\n",
    "    filtered_data['sid'].extend([sid] * len(valid_times))\n",
    "    filtered_data['time'].extend(valid_times)\n",
    "\n",
    "    # add parameters' data      \n",
    "    for var_name in variables_to_extract:\n",
    "        var_data = ds.variables[var_name][:]\n",
    "        fill_value = ds.variables[var_name].getncattr('_FillValue') if '_FillValue' in ds.variables[var_name].ncattrs() else None\n",
    "\n",
    "        filtered_values = var_data[storm_idx, time_mask]\n",
    "\n",
    "        \n",
    "        filtered_values = np.where(filtered_values == fill_value, np.nan, filtered_values)\n",
    "        filtered_values = np.array(filtered_values, dtype=float)\n",
    "        filtered_values = filtered_values.astype(float)                    \n",
    "        filtered_values = np.round(filtered_values, 6)\n",
    "        filtered_data[var_name].extend(filtered_values)\n",
    "\n",
    "filtered_df = pd.DataFrame(filtered_data)\n",
    "\n",
    "filtered_df.to_csv(\"storms_mlc.csv\", index=False, na_rep=\"\")\n",
    "\n",
    "filtered_df['distance_to_shenzhen'] = haversine(filtered_df['lon'], filtered_df['lat'], \n",
    "                                                shenzhen_lon, shenzhen_lat)\n",
    "\n",
    "\n",
    "filtered_df_shenzhen = filtered_df[filtered_df['distance_to_shenzhen'] <= 500]\n",
    "\n",
    "cols_to_check = ['mlc_wind', 'mlc_pres']\n",
    "\n",
    "\n",
    "filtered_df_deleted = filtered_df_shenzhen[~(filtered_df_shenzhen[cols_to_check] == -9999.0).any(axis=1)]\n",
    "filtered_df_deleted.to_csv('storms_mlc_sz.csv', index=False)\n",
    "\n",
    "filtered_df_deleted['time'] = filtered_df_deleted['time'].astype(str)\n",
    "filtered_df_deleted['time'] = pd.to_datetime(filtered_df_deleted['time'], errors='coerce')\n",
    "# yyyy-MM-dd HH:mm:ss\n",
    "filtered_df_deleted['time'] = filtered_df_deleted['time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "filtered_df_deleted.to_csv(\"storms_mlc_formatted.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
